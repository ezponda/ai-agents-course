{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Project 4: Deploy to Production\n\nYour workflows work locally. This project takes a simple AI chat workflow and deploys it to a server so it runs 24/7 ‚Äî anyone with the link can chat with your AI.\n\n**What you'll do:**\n1. Build a minimal chat workflow (Chat Trigger ‚Üí LLM ‚Üí Output)\n2. Deploy it to Railway (~$5/month)\n3. Share the chat link with anyone\n\n**What you'll learn:**\n- How to deploy n8n to a cloud server\n- How to activate workflows for production\n- How to configure `WEBHOOK_URL` so the chat works publicly"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "---\n\n## The Workflow\n\nThe simplest useful production workflow: a chat interface powered by an LLM. Users type a question, the LLM answers, and the response appears in the chat.\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ When chat message      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Answer         ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Output ‚îÇ\n‚îÇ received (Chat Trigger)‚îÇ     ‚îÇ Question (LLM) ‚îÇ     ‚îÇ        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                     ‚îä\n                               OpenRouter Chat Model\n```\n\n> **Import via URL** (copy and paste in n8n ‚Üí Import from URL):\n> ```\n> https://raw.githubusercontent.com/ezponda/ai-agents-course/main/courses/n8n_no_code/book/_static/workflows/13_deploy_to_production.json\n> ```\n>\n> **Download:** {download}`13_deploy_to_production.json <_static/workflows/13_deploy_to_production.json>`\n\n::::{dropdown} üõ†Ô∏è Build this workflow from scratch (step-by-step)\n:color: secondary\n\n### Step 1: Create a new workflow\n\nCreate a new workflow and name it **Deploy to Production**.\n\n### Step 2: Add a Chat Trigger\n\nAdd a **When chat message received** node and configure it:\n\n| Parameter | Value |\n|-----------|-------|\n| **Public** | ON |\n| **Response Mode** | When Last Node Finishes |\n\n### Step 3: Add a Basic LLM Chain\n\nAdd a **Basic LLM Chain** node:\n\n| Parameter | Value |\n|-----------|-------|\n| **Prompt Type** | Define below |\n| **Text** | `{{ $json.chatInput }}` |\n\nSystem message:\n```\nYou are a helpful assistant. Answer the user's question in 2-3 sentences. Be concise and accurate.\n```\n\n### Step 4: Add the Chat Model\n\nAdd an **OpenRouter Chat Model** (or your preferred provider) and connect it to the LLM Chain.\n\n| Parameter | Value |\n|-----------|-------|\n| **Model** | `openai/gpt-4o-mini` |\n\n### Step 5: Add Output node\n\nAdd an **Edit Fields (Set)** node and configure it:\n\n| Name | Type | Value |\n|------|------|-------|\n| `output` | String | `{{ $json.text }}` |\n\nThe Chat Trigger expects the last node to have a field called `output`. The LLM Chain outputs `text`, so this node renames it.\n\n### Step 6: Connect everything\n\n**When chat message received** ‚Üí **Answer Question** ‚Üí **Output**\n\n::::"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## Test Locally First\n\nBefore deploying, make sure the workflow works on your machine.\n\n1. Open the workflow in the editor\n2. Click the **Chat** button in the bottom-right corner\n3. Type a question:\n\n```\nWhat is n8n?\n```\n\nYou should see the LLM's response in the chat window. If this works, you're ready to deploy.\n\n```{note}\nThe Chat Trigger creates two URLs: a **test URL** (works only in the editor) and a **production URL** (works only when the workflow is activated on a server). Locally, you test using the Chat button.\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "---\n\n## Option A: Deploy on Railway\n\n[Railway](https://railway.com/) is a cloud platform that deploys n8n in one click. It costs **~$5/month minimum** (Hobby plan ‚Äî includes $5 of usage credits). No server management needed. See [Railway pricing](https://railway.com/pricing) for current details.\n\n### Step 1: Create a Railway account\n\nGo to [railway.com](https://railway.com/) and sign up (GitHub login works).\n\n### Step 2: Deploy n8n from template\n\n1. Go to [railway.com/template/n8n](https://railway.com/template/n8n)\n2. Click **Deploy Now**\n3. Railway creates a project with n8n + a PostgreSQL database automatically\n4. Wait 2‚Äì3 minutes for the deployment to finish\n\n### Step 3: Open your n8n instance\n\nOnce deployed, Railway gives you a public URL like `your-app.up.railway.app`. Click it to open your n8n instance. Create your admin account on first login.\n\n### Step 4: Set the Webhook URL\n\nThis is the most important production setting. Without it, n8n generates URLs pointing to `localhost` and the chat won't be accessible from outside.\n\n1. In Railway, click on your n8n service\n2. Go to **Variables**\n3. Add (or verify) this environment variable:\n\n| Variable | Value |\n|----------|-------|\n| `WEBHOOK_URL` | `https://your-app.up.railway.app/` |\n\nReplace `your-app.up.railway.app` with your actual Railway URL. **Include the trailing slash.**\n\nRailway will redeploy automatically after adding the variable.\n\n### Step 5: Import and configure the workflow\n\n1. In your Railway n8n instance, go to **Workflows** ‚Üí **Import from URL**\n2. Paste the import URL:\n   ```\n   https://raw.githubusercontent.com/ezponda/ai-agents-course/main/courses/n8n_no_code/book/_static/workflows/13_deploy_to_production.json\n   ```\n3. Configure the **OpenRouter Chat Model** node with your API credential\n\n### Step 6: Activate the workflow\n\nToggle the switch in the top-right corner from **OFF** to **ON**. This is essential ‚Äî the Chat Trigger only works in production when the workflow is active.\n\n### Step 7: Open the chat in your browser\n\nOnce activated, click on the **When chat message received** node and copy the **Production URL**. Open it in any browser ‚Äî you should see a chat interface. Type a question and get an answer from the LLM.\n\n**Share this link with anyone** ‚Äî they can chat with your AI without installing anything.\n\n### Step 8: Check Execution History\n\nIn your Railway n8n instance, go to **Left sidebar ‚Üí Executions**. You should see each chat message listed as a successful execution."
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "---\n\n## Option B: n8n Cloud\n\n[n8n Cloud](https://n8n.io/cloud/) is n8n's official hosted service. It's the easiest option ‚Äî no deployment, no configuration.\n\n| | Details |\n|---|---|\n| **Price** | From ~‚Ç¨20/month (Starter plan, [see pricing](https://n8n.io/pricing/)) |\n| **Includes** | Execution limits vary by plan |\n| **Setup** | Sign up ‚Üí start building |\n| **Webhook URL** | Configured automatically |\n\n### How to use it\n\n1. Sign up at [n8n.io/cloud](https://n8n.io/cloud/)\n2. Import the workflow using the same URL\n3. Configure your API credential\n4. Activate the workflow\n5. Click the **When chat message received** node and copy the **Production URL** ‚Äî open it in your browser\n\nThe advantage of n8n Cloud: no `WEBHOOK_URL` to configure, no server to maintain, automatic updates. The disadvantage: it's the most expensive option, and execution limits may matter for high-traffic workflows."
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "---\n\n## Option C: VPS + Docker (Advanced)\n\nIf you're comfortable with Linux and the terminal, you can run n8n on any VPS (Virtual Private Server) using Docker Compose. Providers like Hetzner ($4/mo), DigitalOcean ($6/mo), or Oracle Cloud (free tier) work well.\n\nThis involves:\n- Setting up a server with Docker\n- Running n8n with `docker compose up -d`\n- Configuring a reverse proxy (Caddy or Nginx) for HTTPS\n- Setting `WEBHOOK_URL` to your domain\n\nIt's the cheapest long-term option and gives you full control, but requires server administration knowledge that is outside the scope of this course.\n\nn8n provides a complete guide: [n8n Docker Compose setup](https://docs.n8n.io/hosting/installation/docker/)."
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "---\n\n## What to Watch Out For\n\n| Topic | What to know |\n|-------|-------------|\n| **Chat URL** | The Chat Trigger generates a public URL when the workflow is active. Copy it from the node ‚Äî don't guess the path. |\n| **WEBHOOK_URL** | Must be set on Railway/VPS so n8n generates the correct public URLs. n8n Cloud sets it automatically. |\n| **Activate the workflow** | The toggle must be ON. Without it, the Chat Trigger doesn't work in production. |\n| **Credentials** | API keys are stored per n8n instance. After deploying, you need to re-enter them on the server. |\n| **Execution History** | Your primary debugging tool in production. Check it in **Left sidebar ‚Üí Executions** after the first chat message. |\n| **Costs** | Railway: ~$5/mo. n8n Cloud: from ~‚Ç¨20/mo. LLM API calls are billed separately by your provider. |\n| **`output` field** | The Chat Trigger expects the last node to have a field called `output`. If you use a Basic LLM Chain (which outputs `text`), add a Set node to rename it. |\n| **Public endpoint = open to anyone** | If the chat is public, anyone with the URL can use it ‚Äî and every message costs you LLM tokens. For a real deployment, consider adding authentication (Webhook node supports Basic Auth or Header Auth) or placing the endpoint behind a proxy with rate limiting. |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "---\n\n## Comparison\n\n| | Railway | n8n Cloud | VPS + Docker |\n|---|---|---|---|\n| **Cost** | ~$5/mo minimum | From ~‚Ç¨20/mo | $4‚Äì12/mo |\n| **Setup** | 5 min (template) | 2 min (sign up) | 30‚Äì60 min |\n| **WEBHOOK_URL** | Set manually | Automatic | Set manually |\n| **Server management** | No | No | Yes |\n| **Execution limits** | None (your server) | Varies by plan | None (your server) |\n| **Best for** | This course | Non-technical users | Production at scale |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "---\n\n## Summary\n\nYou've taken a workflow from your laptop to a public server. The key concepts:\n\n1. **Chat Trigger creates a public URL** ‚Äî anyone with the link can chat with your AI\n2. **Workflows must be activated** ‚Äî toggle ON in the top-right corner\n3. **`WEBHOOK_URL` must be set** ‚Äî so n8n generates correct public URLs (except on n8n Cloud)\n4. **Credentials are per instance** ‚Äî re-enter API keys after deploying\n5. **Execution History is your debugger** ‚Äî check it after the first production message\n\nAny workflow from this course that uses a Chat Trigger or Webhook can be deployed the same way. Configure your credentials, set `WEBHOOK_URL`, activate, and you're live."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}