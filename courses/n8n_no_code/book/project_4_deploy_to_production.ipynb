{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Project 4: Deploy to Production\n\nYour workflows work locally. This project takes a simple AI chat workflow and deploys it to a server so it runs 24/7 ‚Äî anyone with the link can chat with your AI.\n\n**What you'll do:**\n1. Build a minimal chat workflow (Chat Trigger ‚Üí LLM ‚Üí Output)\n2. Deploy it to Railway (free trial, then $5/month)\n3. Share the chat link with anyone\n\n**What you'll learn:**\n- How to deploy n8n to a cloud server\n- How to activate workflows for production\n- How to configure `WEBHOOK_URL` so the chat works publicly"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "---\n\n## The Workflow\n\nThe simplest useful production workflow: a chat interface powered by an LLM. Users type a question, the LLM answers, and the response appears in the chat.\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ When chat message      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Answer         ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Output ‚îÇ\n‚îÇ received (Chat Trigger)‚îÇ     ‚îÇ Question (LLM) ‚îÇ     ‚îÇ        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                     ‚îä\n                               OpenRouter Chat Model\n```\n\n> **Import via URL** (copy and paste in n8n ‚Üí Import from URL):\n> ```\n> https://raw.githubusercontent.com/ezponda/ai-agents-course/main/courses/n8n_no_code/book/_static/workflows/13_deploy_to_production.json\n> ```\n>\n> **Download:** {download}`13_deploy_to_production.json <_static/workflows/13_deploy_to_production.json>`\n\n::::{dropdown} üõ†Ô∏è Build this workflow from scratch (step-by-step)\n:color: secondary\n\n### Step 1: Create a new workflow\n\nCreate a new workflow and name it **Deploy to Production**.\n\n### Step 2: Add a Chat Trigger\n\nAdd a **When chat message received** node and configure it:\n\n| Parameter | Value |\n|-----------|-------|\n| **Public** | ON |\n| **Response Mode** | When Last Node Finishes |\n\n### Step 3: Add a Basic LLM Chain\n\nAdd a **Basic LLM Chain** node:\n\n| Parameter | Value |\n|-----------|-------|\n| **Prompt Type** | Define below |\n| **Text** | `{{ $json.chatInput }}` |\n\nSystem message:\n```\nYou are a helpful assistant. Answer the user's question in 2-3 sentences. Be concise and accurate.\n```\n\n### Step 4: Add the Chat Model\n\nAdd an **OpenRouter Chat Model** (or your preferred provider) and connect it to the LLM Chain.\n\n| Parameter | Value |\n|-----------|-------|\n| **Model** | `openai/gpt-4o-mini` |\n\n### Step 5: Add Output node\n\nAdd an **Edit Fields (Set)** node and configure it:\n\n| Name | Type | Value |\n|------|------|-------|\n| `output` | String | `{{ $json.text }}` |\n\nThe Chat Trigger expects the last node to have a field called `output`. The LLM Chain outputs `text`, so this node renames it.\n\n### Step 6: Connect everything\n\n**When chat message received** ‚Üí **Answer Question** ‚Üí **Output**\n\n::::"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## Test Locally First\n\nBefore deploying, make sure the workflow works on your machine.\n\n1. Open the workflow in the editor\n2. Click the **Chat** button in the bottom-right corner\n3. Type a question:\n\n```\nWhat is n8n?\n```\n\nYou should see the LLM's response in the chat window. If this works, you're ready to deploy.\n\n```{note}\nThe Chat Trigger creates two URLs: a **test URL** (works only in the editor) and a **production URL** (works only when the workflow is activated on a server). Locally, you test using the Chat button.\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "---\n\n## Option A: Deploy on Railway\n\n[Railway](https://railway.com/) is a cloud platform that deploys n8n in one click. It costs **$5/month** after a free trial. No server management needed.\n\n### Step 1: Create a Railway account\n\nGo to [railway.com](https://railway.com/) and sign up (GitHub login works).\n\n### Step 2: Deploy n8n from template\n\n1. Go to [railway.com/template/n8n](https://railway.com/template/n8n)\n2. Click **Deploy Now**\n3. Railway creates a project with n8n + a PostgreSQL database automatically\n4. Wait 2‚Äì3 minutes for the deployment to finish\n\n### Step 3: Open your n8n instance\n\nOnce deployed, Railway gives you a public URL like `your-app.up.railway.app`. Click it to open your n8n instance. Create your admin account on first login.\n\n### Step 4: Set the Webhook URL\n\nThis is the most important production setting. By default, n8n generates webhook URLs using `localhost`, which doesn't work on a server.\n\n1. In Railway, click on your n8n service\n2. Go to **Variables**\n3. Add (or verify) this environment variable:\n\n| Variable | Value |\n|----------|-------|\n| `WEBHOOK_URL` | `https://your-app.up.railway.app/` |\n\nReplace `your-app.up.railway.app` with your actual Railway URL. **Include the trailing slash.**\n\nRailway will redeploy automatically after adding the variable.\n\n### Step 5: Import and configure the workflow\n\n1. In your Railway n8n instance, go to **Workflows** ‚Üí **Import from URL**\n2. Paste the import URL:\n   ```\n   https://raw.githubusercontent.com/ezponda/ai-agents-course/main/courses/n8n_no_code/book/_static/workflows/13_deploy_to_production.json\n   ```\n3. Configure the **OpenRouter Chat Model** node with your API credential\n\n### Step 6: Activate the workflow\n\nToggle the switch in the top-right corner from **OFF** to **ON**. This is essential ‚Äî the Chat Trigger only works in production when the workflow is active.\n\n### Step 7: Open the chat in your browser\n\nOnce activated, the Chat Trigger creates a public chat page. Open this URL in any browser:\n\n```\nhttps://your-app.up.railway.app/webhook/chat-production\n```\n\nReplace `your-app.up.railway.app` with your actual Railway URL. You should see a chat interface ‚Äî type a question and get an answer from the LLM.\n\n**Share this link with anyone** ‚Äî they can chat with your AI without installing anything.\n\n### Step 8: Check Execution History\n\nIn your Railway n8n instance, go to **Executions** (left sidebar). You should see each chat message listed as a successful execution."
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "---\n\n## Option B: n8n Cloud\n\n[n8n Cloud](https://n8n.io/cloud/) is n8n's official hosted service. It's the easiest option ‚Äî no deployment, no configuration.\n\n| | Details |\n|---|---|\n| **Price** | From EUR 24/month (Starter plan) |\n| **Includes** | 2,500 executions/month, 5 active workflows |\n| **Setup** | Sign up ‚Üí start building |\n| **Webhook URL** | Configured automatically |\n\n### How to use it\n\n1. Sign up at [n8n.io/cloud](https://n8n.io/cloud/)\n2. Import the workflow using the same URL\n3. Configure your API credential\n4. Activate the workflow\n5. Open the chat URL provided by n8n Cloud (visible in the Chat Trigger node)\n\nThe advantage of n8n Cloud: no `WEBHOOK_URL` to configure, no server to maintain, automatic updates. The disadvantage: it's the most expensive option, and execution limits may matter for high-traffic workflows."
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "---\n\n## Option C: VPS + Docker (Advanced)\n\nIf you're comfortable with Linux and the terminal, you can run n8n on any VPS (Virtual Private Server) using Docker Compose. Providers like Hetzner ($4/mo), DigitalOcean ($6/mo), or Oracle Cloud (free tier) work well.\n\nThis involves:\n- Setting up a server with Docker\n- Running n8n with `docker compose up -d`\n- Configuring a reverse proxy (Caddy or Nginx) for HTTPS\n- Setting `WEBHOOK_URL` to your domain\n\nIt's the cheapest long-term option and gives you full control, but requires server administration knowledge that is outside the scope of this course.\n\nn8n provides a complete guide: [n8n Docker Compose setup](https://docs.n8n.io/hosting/installation/docker/)."
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "---\n\n## What to Watch Out For\n\n| Topic | What to know |\n|-------|-------------|\n| **Chat URL** | The Chat Trigger generates a public URL when the workflow is active. It only works in production ‚Äî not locally. |\n| **WEBHOOK_URL** | Must be set on Railway/VPS so n8n generates the correct public URLs. n8n Cloud sets it automatically. |\n| **Activate the workflow** | The toggle must be ON. Without it, the Chat Trigger doesn't work in production. |\n| **Credentials** | API keys are stored per n8n instance. After deploying, you need to re-enter them on the server. |\n| **Execution History** | Your primary debugging tool in production. Check it after the first chat message. |\n| **Costs** | Railway: $5/mo. n8n Cloud: from EUR 24/mo. LLM API calls are billed separately by your provider. |\n| **`output` field** | The Chat Trigger expects the last node to have a field called `output`. If you use a Basic LLM Chain (which outputs `text`), add a Set node to rename it. |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison\n",
    "\n",
    "| | Railway | n8n Cloud | VPS + Docker |\n",
    "|---|---|---|---|\n",
    "| **Cost** | $5/mo | From EUR 24/mo | $4‚Äì12/mo |\n",
    "| **Setup** | 5 min (template) | 2 min (sign up) | 30‚Äì60 min |\n",
    "| **WEBHOOK_URL** | Set manually | Automatic | Set manually |\n",
    "| **Server management** | No | No | Yes |\n",
    "| **Execution limits** | None (your server) | 2,500/mo (Starter) | None (your server) |\n",
    "| **Best for** | This course | Non-technical users | Production at scale |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "---\n\n## Summary\n\nYou've taken a workflow from your laptop to a public server. The key concepts:\n\n1. **Chat Trigger creates a public URL** ‚Äî anyone with the link can chat with your AI\n2. **Workflows must be activated** ‚Äî toggle ON in the top-right corner\n3. **`WEBHOOK_URL` must be set** ‚Äî so n8n generates correct public URLs (except on n8n Cloud)\n4. **Credentials are per instance** ‚Äî re-enter API keys after deploying\n5. **Execution History is your debugger** ‚Äî check it after the first production message\n\nAny workflow from this course that uses a Chat Trigger or Webhook can be deployed the same way. Configure your credentials, set `WEBHOOK_URL`, activate, and you're live."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}